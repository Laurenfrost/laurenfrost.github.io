<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Azure Databricks 踩坑记录 | Frozen Sphere</title><script src="//unpkg.com/valine/dist/Valine.min.js"></script><link rel="stylesheet" href="/css/arknights.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css"><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 5.0.2"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body style="background-image:url(https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg);"><header><nav><a href="/.">Home</a><a href="/./archives/">Archives</a><a href="/./friends/">Friends</a><a href="/./about/">About</a></nav></header><main><article><div id="post-bg"><div id="post-title"><h1>Azure Databricks 踩坑记录</h1><hr></div><div id="post-content"><p>开宗明义：傻逼 Databricks！真tmd傻逼！！！！！！！！</p>
<blockquote>
<p>无数次的经验告诉我，instance 的内存大小非常重要！！！<br>instance 的数量只能让更多的 job 同时运行，也就是跑得更快。但真正决定程序能不能运行的还是 instance 的内存大小。<br>如果你的 instance 跑不了 spark 划分的最大的一个 job，那整个程序就跑不了。<br>问就是加钱。</p>
</blockquote>
<h2 id="databricks-connect-问题"><a href="#databricks-connect-问题" class="headerlink" title="databricks-connect 问题"></a>databricks-connect 问题</h2><p>不要以为你的程序打包好之后，上传 Azure Databricks 运行也没问题，就可以高枕无忧。databricks-connect 总是能比你想象得更傻逼！</p>
<h3 id="Cluster-Setup"><a href="#Cluster-Setup" class="headerlink" title="Cluster Setup"></a>Cluster Setup</h3><blockquote>
<p>First you need to enable the feature on your Databricks cluster. Your cluster must be using Databricks Runtime 5.1 or higher. In the web UI edit your cluster and add this/these lines to the spark.conf:</p>
<p>spark.databricks.service.server.enabled true<br>If you are using Azure Databricks also add this line:</p>
<p>spark.databricks.service.port 8787<br>(Note the single space between the setting name and value).</p>
<p>Restart your cluster.<br><a target="_blank" rel="noopener" href="https://datathirst.net/blog/2019/3/7/databricks-connect-finally">参考这里</a></p>
</blockquote>
<p>效果大概是这样：</p>
<p><img src="./databricks-cluster-config.png" alt="databricks-cluster-config"></p>
<p><em>在这里你调整了 port 之后，别忘了你<code>databricks-connect configure</code>的时候，相应的 port 也需要调整，不然它连接不上。</em></p>
<h3 id="报错-A-master-URL-must-be-set"><a href="#报错-A-master-URL-must-be-set" class="headerlink" title="报错 A master URL must be set"></a>报错 <code>A master URL must be set</code></h3><p>虽然 Azure Databrick 在针对 Scala 的编程建议里写道:</p>
<blockquote>
<p>The code should use SparkContext.getOrCreate to obtain a Spark context; otherwise, runs of the job will fail.<br>也就是说，类似于写成这样：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>.builder().getOrCreate()<br></code></pre></td></tr></table></figure>
</blockquote>
<p>它说得确实没错，因为当你打包 jar 上传之后，它已经在相应的 spark 环境里了。它是能直接 get 到 Databricks 提供给它的 context 的。但当你使用 databricks-connect 提供的那一套东西的时候，你就只能 “create” 一个 context。  </p>
<p>所以你要是想用它，你就必须指定 <code>master</code> 为 <code>local</code>，就像这样：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>.builder().master(<span class="hljs-string">&quot;local[*]&quot;</span>).getOrCreate()<br></code></pre></td></tr></table></figure>

<p>然后它才会尝试连接到 Azure Databrick 的 cluster。</p>
<h3 id="报错-Result-for-RPC-Some-lost"><a href="#报错-Result-for-RPC-Some-lost" class="headerlink" title="报错 Result for RPC Some lost"></a>报错 <code>Result for RPC Some lost</code></h3><p>完整的报错信息是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: Result for RPC Some(c74963ef-2411-4c05-9c06-bdd2faeba2ff) lost, please retry your request.<br></code></pre></td></tr></table></figure>

<p>网络波动，重试一遍就好。</p>
<h3 id="报错-java-class-may-not-be-present-on-the-remote-cluster"><a href="#报错-java-class-may-not-be-present-on-the-remote-cluster" class="headerlink" title="报错 java class may not be present on the remote cluster"></a>报错 <code>java class may not be present on the remote cluster</code></h3><p>完整的报错信息是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">Exception in thread &quot;main&quot; com.databricks.service.DependencyCheckWarning: The java class &lt;something&gt; may not be present on the remote cluster. It can be found in &lt;something&gt;&#x2F;target&#x2F;scala-2.11&#x2F;classes. To resolve this, package the classes in &lt;something&gt;&#x2F;target&#x2F;scala-2.11&#x2F;classes into a jar file and then call sc.addJar() on the package jar. You can disable this check by setting the SQL conf spark.databricks.service.client.checkDeps&#x3D;false.<br></code></pre></td></tr></table></figure>

<p>这个讲道理我也没有什么好办法。</p>
<ul>
<li><p>思路 1<br>  参考这个<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/60510382/databricks-connect-dependencycheckwarning-the-java-class-may-not-be-present-on">StackOverflow的答案</a>，虽然它看起来并不怎么靠谱。</p>
<p>  具体而言，就是遵从报错信息的指示，把 class 打包成 jar，然后用 <code>sc.addJar()</code> 加载这个 jar。</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala">spark.sparkContext.addJar(<span class="hljs-string">&quot;你的 jar 文件地址&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>  我也不知道它究竟有没有成效，但至少报错变了。</p>
</li>
<li><p>思路 2<br>  这个思路就是遵从报错信息的另一个指示，通过设置来禁用这个 check，不让他报错。</p>
<p>  虽然有点掩耳盗铃之嫌，不过我还是试了一下。毕竟“黑猫白猫，逮住老鼠就是好猫”。</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span><br>    .builder()<br>    .config(<span class="hljs-string">&quot;spark.databricks.service.client.checkDeps&quot;</span>, <span class="hljs-string">&quot;false&quot;</span>)<br>    .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br>    .getOrCreate()<br></code></pre></td></tr></table></figure>

<p>  后来报错确实变了，变成了这样。这下是彻底找不到 class 了。</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs plain">Exception in thread &quot;main&quot; java.io.InvalidClassException: failed to read class descriptor<br>    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1938)<br>    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1830)<br>    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2121)<br>    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1647)<br>    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:483)<br>    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:441)<br>    at org.apache.spark.sql.util.ProtoSerializer.deserializeObject(ProtoSerializer.scala:4289)<br>    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$deserializeContext$1.apply(ProtoSerializer.scala:259)<br>    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$deserializeContext$1.apply(ProtoSerializer.scala:258)<br>    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)<br>    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)<br>    at scala.collection.Iterator$class.foreach(Iterator.scala:891)<br>    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)<br>    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)<br>    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)<br>    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)<br>    at scala.collection.AbstractTraversable.map(Traversable.scala:104)<br>    at org.apache.spark.sql.util.ProtoSerializer.deserializeContext(ProtoSerializer.scala:258)<br>    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$deserializePlan$1.apply(ProtoSerializer.scala:2882)<br>    at org.apache.spark.sql.util.ProtoSerializer$$anonfun$deserializePlan$1.apply(ProtoSerializer.scala:2882)<br>    at scala.Option.foreach(Option.scala:257)<br>    at org.apache.spark.sql.util.ProtoSerializer.deserializePlan(ProtoSerializer.scala:2882)<br>    at com.databricks.service.SparkServiceRPCHandler.com$databricks$service$SparkServiceRPCHandler$$execute0(SparkServiceRPCHandler.scala:454)<br>    at com.databricks.service.SparkServiceRPCHandler$$anonfun$executeRPC0$1.apply(SparkServiceRPCHandler.scala:343)<br>    at com.databricks.service.SparkServiceRPCHandler$$anonfun$executeRPC0$1.apply(SparkServiceRPCHandler.scala:298)<br>    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)<br>    at com.databricks.service.SparkServiceRPCHandler.executeRPC0(SparkServiceRPCHandler.scala:298)<br>    at com.databricks.service.SparkServiceRPCHandler$$anon$3.call(SparkServiceRPCHandler.scala:255)<br>    at com.databricks.service.SparkServiceRPCHandler$$anon$3.call(SparkServiceRPCHandler.scala:251)<br>    at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>    at com.databricks.service.SparkServiceRPCHandler$$anonfun$executeRPC$1.apply(SparkServiceRPCHandler.scala:287)<br>    at com.databricks.service.SparkServiceRPCHandler$$anonfun$executeRPC$1.apply(SparkServiceRPCHandler.scala:267)<br>    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)<br>    at com.databricks.service.SparkServiceRPCHandler.executeRPC(SparkServiceRPCHandler.scala:266)<br>    at com.databricks.service.SparkServiceRPCServlet.doPost(SparkServiceRPCServer.scala:108)<br>    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)<br>    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)<br>    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)<br>    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)<br>    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)<br>    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)<br>    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)<br>    at org.eclipse.jetty.server.Server.handle(Server.java:534)<br>    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)<br>    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)<br>    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)<br>    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)<br>    at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)<br>    at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)<br>    at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)<br>    at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)<br>    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)<br>    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)<br>    at java.lang.Thread.run(Thread.java:748)<br>Caused by: java.lang.ClassNotFoundException: com.optimind.copernicus.spark.DataGetters$$anonfun$2<br>    at java.lang.ClassLoader.findClass(ClassLoader.java:523)<br>    at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)<br>    at java.lang.ClassLoader.loadClass(ClassLoader.java:418)<br>    at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)<br>    at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(ChildFirstURLClassLoader.java:48)<br>    at java.lang.ClassLoader.loadClass(ClassLoader.java:351)<br>    at java.lang.Class.forName0(Native Method)<br>    at java.lang.Class.forName(Class.java:348)<br>    at org.apache.spark.util.Utils$.classForName(Utils.scala:257)<br>    at org.apache.spark.sql.util.ProtoSerializer.org$apache$spark$sql$util$ProtoSerializer$$readResolveClassDescriptor(ProtoSerializer.scala:4298)<br>    at org.apache.spark.sql.util.ProtoSerializer$$anon$4.readClassDescriptor(ProtoSerializer.scala:4286)<br>    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1936)<br>    ... 53 more<br></code></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="报错-cannot-assign-instance-of-scala-None-to-field"><a href="#报错-cannot-assign-instance-of-scala-None-to-field" class="headerlink" title="报错 cannot assign instance of scala.None$ to field"></a>报错 <code>cannot assign instance of scala.None$ to field</code></h3><p>完整的报错信息是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: java.io.IOException: java.lang.ClassCastException: cannot assign instance of scala.None$ to field org.apache.spark.util.AccumulatorMetadata.name of type scala.Option in instance of org.apache.spark.util.AccumulatorMetadata<br></code></pre></td></tr></table></figure>

<p>我也不知道这段天书在说啥，毕竟我没有清楚地了解 spark 的内部工作机理。</p>
<p>只是个人猜测大概是本地的 spark 和 Azure Databricks 的 spark 之间没有协调好。或许我应该调整一下依赖？</p>
<hr>
<p>重整旗鼓，让我们看看这个 <code>AccumulatorMetadata</code> 是个什么鸡巴？<a target="_blank" rel="noopener" href="https://fossies.org/linux/spark/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala">spark 源码</a>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-number">18</span> <span class="hljs-keyword">package</span> org.apache.spark.util<br> ...<br><span class="hljs-number">30</span> <span class="hljs-keyword">private</span>[spark] <span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AccumulatorMetadata</span>(<span class="hljs-params"></span></span><br><span class="hljs-class"><span class="hljs-params">31     id: <span class="hljs-type">Long</span>,</span></span><br><span class="hljs-class"><span class="hljs-params">32     name: <span class="hljs-type">Option</span>[<span class="hljs-type">String</span>],</span></span><br><span class="hljs-class"><span class="hljs-params">33     countFailedValues: <span class="hljs-type">Boolean</span></span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span></span><br><span class="hljs-class">34 </span><br><span class="hljs-class">35 </span><br><span class="hljs-class">36 <span class="hljs-title">/**</span></span><br><span class="hljs-class">37  <span class="hljs-title">*</span> <span class="hljs-title">The</span> <span class="hljs-title">base</span> <span class="hljs-title">class</span> <span class="hljs-title">for</span> <span class="hljs-title">accumulators</span>, <span class="hljs-title">that</span> <span class="hljs-title">can</span> <span class="hljs-title">accumulate</span> <span class="hljs-title">inputs</span> <span class="hljs-title">of</span> <span class="hljs-title">type</span> `<span class="hljs-title">IN</span>`, <span class="hljs-title">and</span> <span class="hljs-title">produce</span> <span class="hljs-title">output</span> <span class="hljs-title">of</span></span><br><span class="hljs-class">38  <span class="hljs-title">*</span> <span class="hljs-title">type</span> `<span class="hljs-title">OUT</span>`.</span><br><span class="hljs-class">39  <span class="hljs-title">*</span></span><br><span class="hljs-class">40  <span class="hljs-title">*</span> `<span class="hljs-title">OUT</span>` <span class="hljs-title">should</span> <span class="hljs-title">be</span> <span class="hljs-title">a</span> <span class="hljs-title">type</span> <span class="hljs-title">that</span> <span class="hljs-title">can</span> <span class="hljs-title">be</span> <span class="hljs-title">read</span> <span class="hljs-title">atomically</span> (<span class="hljs-params">e.g., <span class="hljs-type">Int</span>, <span class="hljs-type">Long</span></span>), <span class="hljs-title">or</span> <span class="hljs-title">thread-safely</span></span><br><span class="hljs-class">41  <span class="hljs-title">*</span> (<span class="hljs-params">e.g., synchronized collections</span>) <span class="hljs-title">because</span> <span class="hljs-title">it</span> <span class="hljs-title">will</span> <span class="hljs-title">be</span> <span class="hljs-title">read</span> <span class="hljs-title">from</span> <span class="hljs-title">other</span> <span class="hljs-title">threads</span>.</span><br><span class="hljs-class">42  <span class="hljs-title">*/</span></span><br><span class="hljs-class">43 <span class="hljs-title">abstract</span> <span class="hljs-title">class</span> <span class="hljs-title">AccumulatorV2</span>[<span class="hljs-type">IN</span>, <span class="hljs-type">OUT</span>] <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span> </span>&#123;<br><span class="hljs-number">44</span>   <span class="hljs-keyword">private</span>[spark] <span class="hljs-keyword">var</span> metadata: <span class="hljs-type">AccumulatorMetadata</span> = _<br><span class="hljs-number">45</span>   <span class="hljs-keyword">private</span>[<span class="hljs-keyword">this</span>] <span class="hljs-keyword">var</span> atDriverSide = <span class="hljs-literal">true</span><br> ...<br><span class="hljs-number">81</span>   <span class="hljs-comment">/**</span><br><span class="hljs-comment">82    * Returns the name of this accumulator, can only be called after registration.</span><br><span class="hljs-comment">83    */</span><br><span class="hljs-number">84</span>   <span class="hljs-keyword">final</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">name</span></span>: <span class="hljs-type">Option</span>[<span class="hljs-type">String</span>] = &#123;<br><span class="hljs-number">85</span>     assertMetadataNotNull()<br><span class="hljs-number">86</span> <br><span class="hljs-number">87</span>     <span class="hljs-keyword">if</span> (atDriverSide) &#123;<br><span class="hljs-number">88</span>       metadata.name.orElse(<span class="hljs-type">AccumulatorContext</span>.get(id).flatMap(_.metadata.name))<br><span class="hljs-number">89</span>     &#125; <span class="hljs-keyword">else</span> &#123;<br><span class="hljs-number">90</span>       metadata.name<br><span class="hljs-number">91</span>     &#125;<br><span class="hljs-number">92</span>   &#125;<br> ...<br></code></pre></td></tr></table></figure>
<p><s>所以这是个 spark-3.0 的 API，而我要的环境是 Databricks 5.5 LTS 对应的是 spark-2.4.3，这一定有问题。</s></p>
<p>是我搞错了，通过查阅<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.3/api/scala/index.html#org.apache.spark.util.AccumulatorV2">文档</a>和<a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/v2.4.3/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala">源码</a>，其实在 2.4.3 里是有这个接口的，基本就是上面这个代码，所以我的这个猜测是错的。(跪了)</p>
<hr>
<p>When I package jar file, those dependences are also copied into the file. And some of them have conflict with runtime env in remote cluster, which leads to the error <code>cannot assign instance of scala.None$ to field</code>. So, configure build.sbt and prevent unnecessary libraries would help. (I hope so.)</p>
<h3 id="报错-Failed-to-add-jar-file"><a href="#报错-Failed-to-add-jar-file" class="headerlink" title="报错 Failed to add jar file"></a>报错 <code>Failed to add jar file</code></h3><p>完整的报错信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs plain">2021-01-19 08:18:06,056 ERROR [main] SparkContext: Failed to add .&#x2F;jars&#x2F;com.optimind.copernicus.spark.jar to Spark environment<br>java.lang.IllegalArgumentException: Error while instantiating &#39;org.apache.spark.sql.internal.SessionStateBuilder&#39;:<br>    at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1178) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:170) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:169) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at scala.Option.getOrElse(Option.scala:121) ~[scala-library-2.11.12.jar:na]<br>    at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:169) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:166) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:198) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession.conf(SparkSession.scala:198) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.service.SparkClient$.clientEnabled(SparkClient.scala:206) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.spark.util.SparkClientContext$.clientEnabled(SparkClientContext.scala:108) ~[spark-core_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.SparkContext.addJarFile$1(SparkContext.scala:1996) [spark-core_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.SparkContext.addJar(SparkContext.scala:2021) [spark-core_2.11-2.4.3.jar:2.4.3]<br>Caused by: java.util.concurrent.TimeoutException: null<br>    at org.spark_project.jetty.client.util.FutureResponseListener.get(FutureResponseListener.java:109) ~[spark-core_2.11-2.4.3.jar:2.4.3]<br>    at org.spark_project.jetty.client.HttpRequest.send(HttpRequest.java:676) ~[spark-core_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.service.DBAPIClient.get(DBAPIClient.scala:81) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.service.DBAPIClient.jsonGet(DBAPIClient.scala:108) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.service.SparkServiceDebugHelper$.validateSparkServiceToken(SparkServiceDebugHelper.scala:130) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.service.SparkClientManager$class.getForCurrentSession(SparkClient.scala:287) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.service.SparkClientManager$.getForCurrentSession(SparkClient.scala:366) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.service.SparkClient$.getServerHadoopConf(SparkClient.scala:245) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at com.databricks.spark.util.SparkClientContext$.getServerHadoopConf(SparkClientContext.scala:222) ~[spark-core_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.SparkContext$$anonfun$hadoopConfiguration$1.apply(SparkContext.scala:317) ~[spark-core_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.SparkContext$$anonfun$hadoopConfiguration$1.apply(SparkContext.scala:312) ~[spark-core_2.11-2.4.3.jar:2.4.3]<br>    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library-2.11.12.jar:na]<br>    at org.apache.spark.SparkContext.hadoopConfiguration(SparkContext.scala:311) [spark-core_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.internal.SharedState.&lt;init&gt;(SharedState.scala:67) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:145) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:145) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at scala.Option.getOrElse(Option.scala:121) ~[scala-library-2.11.12.jar:na]<br>    at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:145) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:144) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:291) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1175) ~[spark-sql_2.11-2.4.3.jar:2.4.3]<br>    ... 13 common frames omitted<br></code></pre></td></tr></table></figure>


<p>网络波动，重试一遍就好。<br>关闭 clash 的 mixin 功能。</p>
<h3 id="报错-cannot-assign-instance"><a href="#报错-cannot-assign-instance" class="headerlink" title="报错 cannot assign instance"></a>报错 <code>cannot assign instance</code></h3><p>完整的报错信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 45.0 failed 4 times, most recent failure: Lost task 5.3 in stage 45.0 (TID 1846, 10.139.64.5, executor 0): java.lang.ClassCastException: cannot assign instance of org.slf4j.impl.Log4jLoggerAdapter to field ch.qos.logback.classic.Logger.parent of type ch.qos.logback.classic.Logger in instance of ch.qos.logback.classic.Logger<br></code></pre></td></tr></table></figure>

<p>我猜大概是我 lib/ 目录下里有个 fat jar 里有 logger 的库。待我删了它。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs plain">21&#x2F;01&#x2F;19 09:16:11 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Bellf\AppData\Local\Temp\spark-ecd10e21-e079-41ac-b327-c88fe36b9c65\userFiles-59b6b2e4-3139-48e7-9836-00b083054018<br>java.io.IOException: Failed to delete: C:\Users\Bellf\AppData\Local\Temp\spark-ecd10e21-e079-41ac-b327-c88fe36b9c65\userFiles-59b6b2e4-3139-48e7-9836-00b083054018\slf4j-log4j12-1.7.16.jar<br>    at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)<br>    at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)<br>    at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)<br>    at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)<br>    at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)<br>    at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1187)<br>    at org.apache.spark.SparkEnv.stop(SparkEnv.scala:159)<br>    at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:2134)<br>    at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1506)<br>    at org.apache.spark.SparkContext.stop(SparkContext.scala:2133)<br>    at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:629)<br>    at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)<br>    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2121)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)<br>    at scala.util.Try$.apply(Try.scala:192)<br>    at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)<br>    at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)<br></code></pre></td></tr></table></figure>

<h2 id="Databricks-Notebook-问题"><a href="#Databricks-Notebook-问题" class="headerlink" title="Databricks Notebook 问题"></a>Databricks Notebook 问题</h2><h3 id="报错-not-found-value-expr"><a href="#报错-not-found-value-expr" class="headerlink" title="报错 not found value expr"></a>报错 <code>not found value expr</code></h3><p>问题代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> nodes = spark.read.format(<span class="hljs-string">&quot;jdbc&quot;</span>)......load()<br>nodes.withColumn(<span class="hljs-string">&quot;shapeWKT&quot;</span>, expr(<span class="hljs-string">&quot;ST_GeomFromWKB(shape)&quot;</span>)).show()<br></code></pre></td></tr></table></figure>

<p>报错内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs log">command-633362346182315:33: error: not found: value expr<br>nodes.withColumn(&quot;shapeWKT&quot;, expr(&quot;ST_GeomFromWKB(shape)&quot;)).show()<br></code></pre></td></tr></table></figure>

<p>解决办法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.functions._<br></code></pre></td></tr></table></figure>

<p>参考资料：<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/36330024/sparksql-not-found-value-expr/36330128">https://stackoverflow.com/questions/36330024/sparksql-not-found-value-expr/36330128</a></p>
<h3 id="报错-Undefined-function-39-ST-GeomFromWKB-39"><a href="#报错-Undefined-function-39-ST-GeomFromWKB-39" class="headerlink" title="报错 Undefined function: &#39;ST_GeomFromWKB&#39;"></a>报错 <code>Undefined function: &#39;ST_GeomFromWKB&#39;</code></h3><p>问题代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.types._<br><span class="hljs-keyword">import</span> org.apache.spark.sql.functions._<br><br><span class="hljs-keyword">val</span> nodes = spark.read.format(<span class="hljs-string">&quot;jdbc&quot;</span>)......load()<br>nodes.withColumn(<span class="hljs-string">&quot;shapeWKT&quot;</span>, expr(<span class="hljs-string">&quot;ST_GeomFromWKB(shape)&quot;</span>)).show()<br></code></pre></td></tr></table></figure>

<p>报错内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs log">org.apache.spark.sql.AnalysisException: Undefined function: &#39;ST_GeomFromWKB&#39;.<br> This function is neither a registered temporary function nor a permanent function registered in the database &#39;default&#39;.; line 1 pos 0<br></code></pre></td></tr></table></figure>

<p>解决办法：<br>在 cluster 的 advanced option 里，给 spark config 添加下列语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">spark.kryo.registrator<br>org.datasyslab.geospark.serde.GeoSparkKryoRegistrator<br></code></pre></td></tr></table></figure>
<p>然后 import 相关库：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark.serializer.<span class="hljs-type">KryoSerializer</span><br><span class="hljs-keyword">import</span> org.datasyslab.geosparkviz.sql.utils.<span class="hljs-type">GeoSparkVizRegistrator</span><br><span class="hljs-keyword">import</span> org.datasyslab.geosparksql.utils.&#123;<span class="hljs-type">Adapter</span>, <span class="hljs-type">GeoSparkSQLRegistrator</span>&#125;<br><span class="hljs-keyword">import</span> org.datasyslab.geosparkviz.core.<span class="hljs-type">Serde</span>.<span class="hljs-type">GeoSparkVizKryoRegistrator</span><br><span class="hljs-type">GeoSparkSQLRegistrator</span>.registerAll(spark)<br><span class="hljs-type">GeoSparkVizRegistrator</span>.registerAll(spark)<br></code></pre></td></tr></table></figure>

<p>参考资料：<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/62830434/st-geomfromtext-function-using-spark-java">https://stackoverflow.com/questions/62830434/st-geomfromtext-function-using-spark-java</a></p>
<h3 id="报错-Can-not-create-the-managed-table"><a href="#报错-Can-not-create-the-managed-table" class="headerlink" title="报错 Can not create the managed table"></a>报错 <code>Can not create the managed table</code></h3><p>简而言之就是不能 overwrite。</p>
<p>报错代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-type">SomeData_df</span>.write.mode(<span class="hljs-string">&quot;overwrite&quot;</span>).saveAsTable(<span class="hljs-string">&quot;SomeData&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>报错内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs log">org.apache.spark.sql.AnalysisException: Can not create the managed table(&#39;SomeData&#39;). <br>The associated location(&#39;dbfs:&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;somedata&#39;) already exists.;<br></code></pre></td></tr></table></figure>

<p>解决办法：<br>明明是 overwrite 模式，但还是不行，真傻逼。</p>
<ul>
<li><p>思路1：直接删除<br>它不是已经存在了吗，删了就完事儿了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala">dbutils.fs.rm(<span class="hljs-string">&quot;dbfs:/user/hive/warehouse/SomeData&quot;</span>, recurse=<span class="hljs-literal">true</span>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>思路2：调整 Databricks 设置</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala">spark.conf.set(<span class="hljs-string">&quot;spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation&quot;</span>,<span class="hljs-string">&quot;true&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>但这个选项在 Spark 3.0.0 中被去掉了。如果用更高版本的 Databricks 集群的话就不行。会有这样的报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs log">Caused by: org.apache.spark.sql.AnalysisException: The SQL config &#39;spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation&#39; was removed in the version 3.0.0.<br>It was removed to prevent loosing of users data for non-default value.;<br></code></pre></td></tr></table></figure>
<p>要解决这个问题，必须写出完整的文件路径才行。</p>
</li>
<li><p>思路3：直接 overwrite 文件的绝对路径。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs scala">df.write \<br>  .option(<span class="hljs-string">&quot;path&quot;</span>, <span class="hljs-string">&quot;hdfs://cluster_name/path/to/my_db&quot;</span>) \<br>  .mode(<span class="hljs-string">&quot;overwrite&quot;</span>) \<br>  .saveAsTable(<span class="hljs-string">&quot;my_db.my_table&quot;</span>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>思路4：修改 cluster 配置<br>这个基本上是思路 2 的加强版。<br>在 cluster 的 advanced option 里，给 spark config 添加下列语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation true<br></code></pre></td></tr></table></figure>

</li>
</ul>
<p>参考资料：  </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-overwrite-cancel">https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/spark-overwrite-cancel</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/55380427/azure-databricks-can-not-create-the-managed-table-the-associated-location-alre">https://stackoverflow.com/questions/55380427/azure-databricks-can-not-create-the-managed-table-the-associated-location-alre</a></li>
</ul>
<h2 id="Databricks-Cluster-问题"><a href="#Databricks-Cluster-问题" class="headerlink" title="Databricks Cluster 问题"></a>Databricks Cluster 问题</h2><h3 id="Java-heap-space-不够"><a href="#Java-heap-space-不够" class="headerlink" title="Java heap space 不够"></a>Java heap space 不够</h3><p><code>java.lang.OutOfMemoryError: Java heap space</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs plain">21&#x2F;02&#x2F;10 10:41:45 WARN TaskSetManager: Lost task 22.2 in stage 13.0 (TID 47942, 10.139.64.12, executor 19): java.lang.OutOfMemoryError: Java heap space<br>    at java.util.HashMap.resize(HashMap.java:704)<br>    at java.util.HashMap.putVal(HashMap.java:663)<br>    at java.util.HashMap.put(HashMap.java:612)<br>    at com.bmwcarit.barefoot.topology.Graph.construct(Graph.java:94)<br>    at com.bmwcarit.barefoot.roadmap.RoadMap.constructNetworkTopology(RoadMap.kt:147)<br>    at com.optimind.copernicus.spark.Main$$anonfun$4.apply(Main.scala:103)<br>    at com.optimind.copernicus.spark.Main$$anonfun$4.apply(Main.scala:86)<br>    at org.apache.spark.sql.execution.MapGroupsExec$$anonfun$10$$anonfun$apply$4.apply(objects.scala:365)<br>    at org.apache.spark.sql.execution.MapGroupsExec$$anonfun$10$$anonfun$apply$4.apply(objects.scala:364)<br>    at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)<br>    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)<br>    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)<br>    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)<br>    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)<br>    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:235)<br>    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)<br>    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:172)<br>    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)<br>    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)<br>    at org.apache.spark.scheduler.Task.run(Task.scala:113)<br>    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$17.apply(Executor.scala:606)<br>    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)<br>    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:612)<br>    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>    at java.lang.Thread.run(Thread.java:748)<br></code></pre></td></tr></table></figure>

<p>解决办法很简单，用更大的instance。</p>
<p><img src="./more-mem-instance.png" alt="你需要更大的instance"></p>
<p>— 分界线 —</p>
<p>2021-08-24 更新：</p>
<p>不花钱怎么能跑程序？</p>
<p>无数次的经验告诉我，instance 的内存大小非常重要！！！</p>
<p>instance 的数量只能让更多的 job 同时运行，也就是跑得更快。但真正决定程序能不能运行的还是 instance 的内存大小。</p>
<p>如果你的 instance 跑不了 spark 划分的最大的一个 job，那整个程序就跑不了。</p>
<p><img src="./more-and-more-mem-instance.png" alt="更多更多内存"></p>
<div id="paginator"></div></div><div id="post-footer"><hr><a href="/2021/03/03/gradle-hotspot-conflict/">← Prev Windows 10 的热点与 Gradle 冲突</a><span style="color: #fe2"> | </span><a href="/2020/10/22/ocl-context/">OpenCL 工作模型 Next →</a><hr></div><div id="bottom-btn"><a id="to-index" href="#post-index" title="index">≡</a><a id="to-top" href="#post-title" title="to top">∧</a></div><div id="Valine"></div><script>new Valine({
 el: '#Valine'
 , appId: 'uyrmyOMoruPODMfC9D5LwLLo-gzGzoHsz'
 , appKey: 'WCEOpH3VI138eAxVITivdGSv'
 , placeholder: '此条评论委托企鹅物流发送'
})</script></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/"> Dr.Laurenfrost</a></h1><section id="total"><a id="total-archives" href="/archives"><span class="total-title">Archives Total:</span><span class="total-number">19</span></a><div id="total-tags"><span class="total-title">Tags:</span><span class="total-number">17</span></div><div id="total-categories"><span class="total-title">Categories:</span><span class="total-number">10</span></div></section></div><div id="aside-block"><h1>INDEX</h1><div id="post-index"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#databricks-connect-%E9%97%AE%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">databricks-connect 问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cluster-Setup"><span class="toc-number">1.1.</span> <span class="toc-text">Cluster Setup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-A-master-URL-must-be-set"><span class="toc-number">1.2.</span> <span class="toc-text">报错 A master URL must be set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-Result-for-RPC-Some-lost"><span class="toc-number">1.3.</span> <span class="toc-text">报错 Result for RPC Some lost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-java-class-may-not-be-present-on-the-remote-cluster"><span class="toc-number">1.4.</span> <span class="toc-text">报错 java class may not be present on the remote cluster</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-cannot-assign-instance-of-scala-None-to-field"><span class="toc-number">1.5.</span> <span class="toc-text">报错 cannot assign instance of scala.None$ to field</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-Failed-to-add-jar-file"><span class="toc-number">1.6.</span> <span class="toc-text">报错 Failed to add jar file</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-cannot-assign-instance"><span class="toc-number">1.7.</span> <span class="toc-text">报错 cannot assign instance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Databricks-Notebook-%E9%97%AE%E9%A2%98"><span class="toc-number">2.</span> <span class="toc-text">Databricks Notebook 问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-not-found-value-expr"><span class="toc-number">2.1.</span> <span class="toc-text">报错 not found value expr</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-Undefined-function-39-ST-GeomFromWKB-39"><span class="toc-number">2.2.</span> <span class="toc-text">报错 Undefined function: &#39;ST_GeomFromWKB&#39;</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%A5%E9%94%99-Can-not-create-the-managed-table"><span class="toc-number">2.3.</span> <span class="toc-text">报错 Can not create the managed table</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Databricks-Cluster-%E9%97%AE%E9%A2%98"><span class="toc-number">3.</span> <span class="toc-text">Databricks Cluster 问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Java-heap-space-%E4%B8%8D%E5%A4%9F"><span class="toc-number">3.1.</span> <span class="toc-text">Java heap space 不够</span></a></li></ol></li></ol></div></div><footer><nobr><span class="text-title">©</span><span class="text-content">2020 to ????</span></nobr><wbr><nobr><span class="text-title">ICP</span><span class="text-content">备案个屁</span></nobr><wbr><nobr><span class="text-title">Auther</span><span class="text-content">Laurenfrost</span></nobr><wbr><wbr><nobr>published with&nbsp;<a target="_blank" rel="noopener" href="http://hexo.io">Hexo&nbsp;</a></nobr><wbr><nobr>Theme&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknight&nbsp;</a></nobr><wbr><nobr>by&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/arknights.js"></script><script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script></body></html>